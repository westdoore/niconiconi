import csv
import datetime
import json
import os
import random
import shutil
import subprocess
import sys
import time
import uuid
from collections import namedtuple
from redis_tool import r
import pytz

import notify
from asn import Wanted_ASN, ASN_Map
import redis
import requests

from log import logger


def acquire_lock_with_timeout(redis_client, lock_name, acquire_timeout=60 * 60, lock_timeout=60 * 60):
    identifier = str(uuid.uuid4())
    end = time.time() + acquire_timeout
    while time.time() < end:
        if redis_client.set(lock_name, identifier, nx=True, ex=lock_timeout):
            return identifier
        time.sleep(0.001)
    return False


def release_lock(redis_client, lock_name, identifier):
    while True:
        try:
            with redis_client.pipeline() as pipe:
                pipe.watch(lock_name)
                lock_value = redis_client.get(lock_name)
                if lock_value and lock_value.decode('utf-8') == identifier:
                    pipe.multi()
                    pipe.delete(lock_name)
                    pipe.execute()
                    return True
                pipe.unwatch()
                break
        except redis.WatchError:
            continue
    return False


# è·å–æ‰€æœ‰ CIDR åˆ—è¡¨
def get_cidr_ips(asn):
    # ç¡®ä¿ asn ç›®å½•å­˜åœ¨
    asn_dir = "asn"
    os.makedirs(asn_dir, exist_ok=True)

    file_path = os.path.join(asn_dir, f"{asn}")

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„ ASN æ–‡ä»¶
    if os.path.exists(file_path):
        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r') as file:
            cidrs = json.load(file)
        print(f"CIDR data for ASN {asn} loaded from file.")
    else:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ±‚ API æ•°æ®
        url = f'https://api.bgpview.io/asn/{asn}/prefixes'
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
            "Cookie": "cf_clearance=QGTGcYnHuiA.9rho9oE4t8qMiyEOZbTbSISclJRmL2A-1720255983-1.0.1.1-Mf0yAeogUfsanJBjw3qpZKalVLAfsN8AyPnjlQDzT0PvEFBOO7Ypp9NyQ4WCWHIAaeCAYaqpVE_Aa6z3s8AIpA; _ga=GA1.2.16443840.1721715301; _gid=GA1.2.1729940749.1721936545; _ga_7YFHLCZHVM=GS1.2.1721936545.5.1.1721937177.55.0.0"
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()
        cidrs = [prefix['prefix'] for prefix in data['data']['ipv4_prefixes']]

        # å°†æ•°æ®å†™å…¥æ–‡ä»¶
        with open(file_path, 'w') as file:
            json.dump(cidrs, file)
        print(f"CIDR data for ASN {asn} fetched from API and saved to file.")

    return cidrs


# å°† CIDR åˆ—è¡¨å­˜å…¥ Redis
def store_cidrs_in_redis(asn, batch_ip_size):
    cidrs = get_cidr_ips(asn)

    def ip_count(cidr):
        ip, mask = cidr.split('/')
        mask = int(mask)
        return 2 ** (32 - mask) if mask < 32 else 1

    total_ips = sum(ip_count(cidr) for cidr in cidrs)

    if total_ips <= batch_ip_size:
        r.rpush(f"cidr_batches:{asn}", json.dumps(cidrs))
    else:
        batches = []
        current_batch = []
        current_batch_ip_count = 0
        for cidr in cidrs:
            cidr_ip_count = ip_count(cidr)
            if current_batch_ip_count + cidr_ip_count > batch_ip_size and current_batch:
                batches.append(current_batch)
                current_batch = []
                current_batch_ip_count = 0
            current_batch.append(cidr)
            current_batch_ip_count += cidr_ip_count

        if current_batch:
            batches.append(current_batch)

        # å¦‚æœæ‰¹æ¬¡æ•°é‡å¤§äº 10ï¼Œå‡åŒ€åˆ†æˆåä»½
        if len(batches) > 10:
            total_cidrs = [cidr for batch in batches for cidr in batch]
            chunk_size = len(total_cidrs) // 10
            batches = [total_cidrs[i * chunk_size: (i + 1) * chunk_size] for i in range(10)]
            if len(total_cidrs) % 10 != 0:
                for i in range(len(total_cidrs) % 10):
                    batches[i].append(total_cidrs[-(i + 1)])

        for batch in batches:
            r.rpush(f"cidr_batches:{asn}", json.dumps(batch))


def ip_count(cidr):
    ip, mask = cidr.split('/')
    mask = int(mask)
    return 2 ** (32 - mask) if mask < 32 else 1


def split_large_batches(batches, batch_ip_size):
    new_batches = []
    for batch in batches:
        if len(new_batches) >= 10:
            new_batches.append(batch)
            continue
        current_batch = []
        current_batch_ip_count = 0
        for cidr in batch:
            cidr_ip_count = ip_count(cidr)
            if current_batch_ip_count + cidr_ip_count > batch_ip_size and current_batch:
                new_batches.append(current_batch)
                current_batch = []
                current_batch_ip_count = 0
                if len(new_batches) >= 10:
                    break
            current_batch.append(cidr)
            current_batch_ip_count += cidr_ip_count
        if current_batch:
            new_batches.append(current_batch)
        if len(new_batches) >= 10:
            break
    return new_batches


# è·å– CIDR æ‰¹æ¬¡
def get_cidr_batch(asn):
    cidr_batch = r.lpop(f"cidr_batches:{asn}")
    if cidr_batch:
        return json.loads(cidr_batch)
    return []


# ä½¿ç”¨ Masscan æ‰«ææ‰€æœ‰ IP çš„ç«¯å£
def scan_ip_range(cidr, output_file, scan_ports="443"):
    cmd = ["masscan", cidr, f"-p{scan_ports}", "--rate=20000", "--wait=3", "-oL", output_file]
    print(f"Executing command: {' '.join(cmd)}")
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print("Scan completed successfully.")
        print(result.stdout)
    except subprocess.CalledProcessError as e:
        print(f"Error executing masscan: {e}")
        print(f"Exit status: {e.returncode}")
        print(f"Standard output: {e.stdout}")
        print(f"Standard error: {e.stderr}")


def iptest_snifferx(input_file: str, output_file: str) -> str | None:
    # ./iptest -file=ip.txt -max=100 -outfile=AS4609-20000-25000.csv -speedtest=3 -tls=1
    cmd = ["./love-you", f"-file={input_file}", f"-max=100", f"-outfile={output_file}", "-speedtest=3", "-tls=1"]
    print(f"Executing command: {' '.join(cmd)}")
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print("IPTest completed successfully.")
        print(result.stdout)
    except subprocess.CalledProcessError as e:
        print(f"Error executing masscan: {e}")
        print(f"Exit status: {e.returncode}")
        print(f"Standard output: {e.stdout}")
        print(f"Standard error: {e.stderr}")
    if os.path.exists(output_file):
        return output_file
    return None


# è§£æ Masscan è¾“å‡ºå¹¶ç»Ÿè®¡ç«¯å£
def parse_masscan_output(file_path: str, ip_text_file: str):
    ip_port_list = []
    with open(file_path, 'r') as file:
        for line in file:
            if line.startswith('open'):
                parts = line.split()
                if len(parts) >= 3:
                    port = parts[2]
                    ip = parts[3]
                    ip_port_list.append(ip + " " + port)
    with open(ip_text_file, "w") as f:
        f.write("\n".join(ip_port_list))
        f.flush()
    return ip_text_file


# å°†ç«¯å£ç»Ÿè®¡ç»“æœå­˜å‚¨åˆ° Redis
# def store_ip_port_result_in_redis(asn, iptests:[]):
#     lock_name = f"lock:snifferx-result:{asn}"
#     identifier = acquire_lock_with_timeout(r, lock_name)
#
#     if identifier:
#         try:
#             for server in iptests:
#                 ip = server['ip']
#                 port = server['port']
#                 server_info_json = json.dumps(server)
#                 r.hsetnx('snifferx-result', f'{asn}:{ip}:{port}', server_info_json)
#         finally:
#             release_lock(r, lock_name, identifier)
#     else:
#         print("Failed to acquire lock for updating port_counts")


def store_ip_port_result_in_redis(asn, iptests: []):
    for server in iptests:
        ip = server["ip"]
        port = server["port"]
        # TODO åˆ¤æ–­æ˜¯å¦æœ‰é—®é¢˜ 0.00 kB/s
        # ä¿®æ”¹ä¸º0.00 å¯èƒ½ä¼šé€ æˆä¸€äº›èƒ½ç”¨çš„IPè¢«é—æ¼
        # å¦‚æœè®¾ç½®ä¸º0.0 ä¼šå¯¼è‡´æœ‰äº›çœ‹ä¼¼å¯ç”¨çš„IPè¢«è¯¯ç”¨
        # ç›®å‰è®¾ç½®ä¸ºå®æ„¿é—æ¼
        if server["download_speed"] == '0.00 kB/s':
            continue
        server_info_json = json.dumps(server)

        r.hsetnx('snifferx-result', f'{asn}:{ip}:{port}', server_info_json)


def server_info_to_dict(server_info):
    return {
        "ip": server_info.ip,
        "port": server_info.port,
        "enable_tls": server_info.enable_tls,
        "data_center": server_info.data_center,
        "region": server_info.region,
        "city": server_info.city,
        "network_latency": server_info.network_latency,
        "download_speed": server_info.download_speed
    }


def scan_and_store_results(asn, scan_ports):
    os.makedirs("masscan_results", exist_ok=True)
    while True:
        batch = get_cidr_batch(asn)
        if not batch:
            break
        cidrs = " ".join(batch)
        output_file = f"masscan_results/{batch[0].replace('/', '-')}_temp.txt"
        scan_ip_range(cidrs, output_file, scan_ports)
        ip_text_file = f"masscan_results/{batch[0].replace('/', '-')}_ip.txt"
        ip_port_file = parse_masscan_output(output_file, ip_text_file)
        ip_test_file = f"masscan_results/{batch[0].replace('/', '-')}_iptest.csv"
        snifferx = iptest_snifferx(ip_port_file, ip_test_file)
        if snifferx:
            # parse result and store to redis
            iptests = parse_result_csv(snifferx)
            store_ip_port_result_in_redis(asn, iptests)

        time.sleep(3)  # ç­‰å¾…ä¸€ä¼šå„¿å†è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡

    print(f"å½“å‰èŠ‚ç‚¹ä»»åŠ¡å·²ç»å®Œæˆ: {datetime.datetime.now()}")
    clear_directory("masscan_results")


# æœ€å¤šè¿”å›6è¡Œæ•°æ®
def parse_result_csv(result_csv_file: str) -> []:
    ServerInfo = namedtuple("ServerInfo", ["ip", "port", "enable_tls", "data_center",
                                           "region", "city", "network_latency", "download_speed"])

    with open(result_csv_file, 'r') as file:
        reader = csv.reader(file)
        next(reader)  # Skip the header row

        data = []
        for row in reader:
            server_info = ServerInfo(
                ip=row[0],
                port=int(row[1]),
                enable_tls=row[2].lower() == "true",
                data_center=row[3],
                region=row[4],
                city=row[5],
                network_latency=row[6],
                download_speed=row[7]
            )
            server_info_dict = server_info_to_dict(server_info)
            data.append(server_info_dict)
    # TODO ä»¥HK JP TW KR SG æ’åº
    return data if len(data) < 6 else data[:6]


def clear_directory(folder_path):
    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    if os.path.exists(folder_path):
        # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å†…å®¹
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            try:
                # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œåˆ™é€’å½’åˆ é™¤
                if os.path.isdir(file_path):
                    shutil.rmtree(file_path)
                # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œåˆ™ç›´æ¥åˆ é™¤
                else:
                    os.remove(file_path)
            except Exception as e:
                print(f'Error: {e}')


def clean_duplicate_redis_data(asn: str):
    clean_key = f"clean_lock:{asn}"
    initialized_key = f"task_initialized:{asn}"
    exists = r.exists(initialized_key)
    if exists:
        return
        # ä½¿ç”¨ Redis çš„åŸå­æ“ä½œ set é…åˆ NX é€‰é¡¹
    if r.set(clean_key, "1", nx=True):
        try:
            keys_to_delete = r.keys(f'*{asn}*')

            # åˆ é™¤è¿™äº›é”®
            if keys_to_delete:
                r.delete(*keys_to_delete)
        except Exception as e:
            # å¦‚æœåˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œåˆ é™¤æ ‡è®°é”®ä»¥å…è®¸é‡è¯•
            r.delete(clean_key)
    else:
        print(f"Redisæ•°æ®å·²è¢«å…¶ä»–æœåŠ¡å™¨æ¸…ç† {asn}")


def initialize_task(asn, batch_ip_size):
    initialized_key = f"task_initialized:{asn}"

    # ä½¿ç”¨ Redis çš„åŸå­æ“ä½œ set é…åˆ NX é€‰é¡¹
    if r.set(initialized_key, "1", nx=True):
        try:
            store_cidrs_in_redis(asn, batch_ip_size)
            print(f"Task initialized for ASN {asn}")
        except Exception as e:
            # å¦‚æœåˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œåˆ é™¤æ ‡è®°é”®ä»¥å…è®¸é‡è¯•
            r.delete(initialized_key)
            print(f"Error initializing task for ASN {asn}: {e}")
            raise
    else:
        print(f"Task already initialized for ASN {asn}")


def mark_task_completed(asn, num_instances):
    lock_name = f"completion_lock:{asn}"
    identifier = acquire_lock_with_timeout(r, lock_name)
    if identifier:
        try:
            completed_key = f"completed_instances:{asn}"
            completed_instances = int(r.get(completed_key) or 0)
            if completed_instances < num_instances:
                r.incr(completed_key)
                logger.info("ä»»åŠ¡å·²å®Œæˆ...")
            else:
                logger.info("æ‰€æœ‰å®ä¾‹å·²ç»å®Œæˆä»»åŠ¡ï¼Œä¸éœ€è¦å†å¢åŠ è®¡æ•°")
        finally:
            release_lock(r, lock_name, identifier)


def is_task_completed(asn, num_instances):
    lock_name = f"lock:task_check:{asn}"
    identifier = acquire_lock_with_timeout(r, lock_name, acquire_timeout=10, lock_timeout=10)

    if not identifier:
        logger.warning(f"Failed to acquire lock for task check for ASN {asn}")
        return False

    try:
        completed_key = f"completed_instances:{asn}"
        completed_instances = int(r.get(completed_key) or 0)
        logger.info(f"Task completed: {completed_instances} instances")
        return completed_instances >= num_instances
    finally:
        release_lock(r, lock_name, identifier)


def count_fields_containing_asn(hashmap_key, asn):
    count = 0
    cursor = 0

    while True:
        # ä½¿ç”¨ HSCAN å‘½ä»¤è·å–ä¸€æ‰¹ field
        cursor, fields = r.hscan(hashmap_key, cursor)

        # è®¡ç®—åŒ…å« 'abc' çš„ field æ•°é‡
        count += sum(1 for field in fields if f'{asn}' in str(field))

        # å¦‚æœ cursor ä¸º 0ï¼Œè¯´æ˜éå†å®Œæˆ
        if cursor == 0:
            break

    return count


def run_task(asn_number: str):
    asn = asn_number
    clean_duplicate_redis_data(asn)
    # scan_ports = (
    #     '443,1443,2443,3443,4443,5443,6443,7443,8443,9443,'
    #     '10443,11443,12443,13443,14443,15443,16443,17443,18443,19443,'
    #     '20443,21443,22443,23443,24443,25443,26443,27443,28443,29443,'
    #     '30443,31443,32443,33443,34443,35443,36443,37443,38443,39443,'
    #     '40443,41443,42443,43443,44443,45443,46443,47443,48443,49443,'
    #     '50443,51443,52443,53443,54443,55443,56443,57443,58443,59443,'
    #     '60443,61443,62443,63443,64443,65443,23555')

    scan_ports = '443,8443,9443,23555'
    batch_ip_size = 100000  # Example batch size

    # åˆå§‹åŒ–ä»»åŠ¡ï¼Œåªéœ€æ‰§è¡Œä¸€æ¬¡
    initialize_task(asn, batch_ip_size)

    # ç­‰å¾…åç§’
    time.sleep(random.randint(1, 10))

    scan_and_store_results(asn, scan_ports)

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å®ä¾‹éƒ½å®Œæˆä»»åŠ¡
    num_instances = 10  # å‡è®¾æœ‰åå°æœºå™¨
    # æ ‡è®°ä»»åŠ¡å®Œæˆ
    mark_task_completed(asn, num_instances)

    while True:
        if is_task_completed(asn, num_instances):
            # å¦‚æœæ˜¯æœ€åä¸€å°å®Œæˆçš„æœºå™¨ï¼Œåˆ™ç”Ÿæˆå›¾è¡¨å’Œåˆ·æ–° Markdown
            if r.incr(f"last_instance:{asn}") == 1:
                result_counts = count_fields_containing_asn("snifferx-result", asn)
                msg_info = f"æ‰«æç»“æŸ: ASN{asn},ç»“æœæ•°é‡: {result_counts}"
                telegram_notify = notify.pretty_telegram_notify("ğŸ‰ğŸ‰Open-Port-Snifferè¿è¡Œç»“æŸ",
                                                                f"open-port-sniffer asn{asn}",
                                                                msg_info)
                telegram_notify = notify.clean_str_for_tg(telegram_notify)
                success = notify.send_telegram_message(telegram_notify)

                if success:
                    print("Finish scan message sent successfully!")
                else:
                    print("Finish scan message failed to send.")
            break
        logger.info(f"ç­‰å¾…å…¶ä»–èŠ‚ç‚¹å®Œæˆä»»åŠ¡(ç¡çœ 10s)...")
        time.sleep(10)


def delete_keys_containing_asn(hashmap_key, asn):
    # è·å– hashmap ä¸­çš„æ‰€æœ‰ key
    all_keys = r.hkeys(hashmap_key)

    # ç­›é€‰å‡ºåŒ…å« 'abc' çš„ key
    keys_to_delete = [key for key in all_keys if asn in str(key)]

    # å¦‚æœæœ‰éœ€è¦åˆ é™¤çš„ key
    if keys_to_delete:
        # ä½¿ç”¨ HDEL å‘½ä»¤åˆ é™¤è¿™äº› key
        r.hdel(hashmap_key, *keys_to_delete)
        print(f"Deleted {len(keys_to_delete)} keys containing asn'{asn}'")
    else:
        print(f"No keys containing asn '{asn}' found")


def get_current_weekday():
    # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
    current_date = datetime.datetime.now()

    # è·å–å½“å‰æ˜¯æ˜ŸæœŸå‡ ï¼ˆ0æ˜¯å‘¨ä¸€ï¼Œ6æ˜¯å‘¨æ—¥ï¼‰
    weekday = current_date.weekday()

    # å¦‚æœæ˜¯å‘¨æ—¥ï¼ˆåŸæœ¬è¿”å›6ï¼‰ï¼Œæˆ‘ä»¬ä¿æŒä¸å˜
    # å…¶ä»–å¤©æ•°ä¿æŒä¸å˜ï¼ˆå‘¨ä¸€æ˜¯0ï¼Œå‘¨äºŒæ˜¯1ï¼Œä»¥æ­¤ç±»æ¨ï¼‰
    return weekday


# ä¿®æ”¹ä¸ºé»˜è®¤ç¾å›½ä¸œéƒ¨æ—¶é—´
def get_current_weekday_plus():
    # Define the US Eastern time zone
    eastern = pytz.timezone('US/Eastern')

    # Get the current time in the US Eastern time zone
    now = datetime.datetime.now(eastern)
    current_time = now.time()
    current_day = now.weekday()  # Monday is 0, Sunday is 6

    # Define time ranges
    morning_start = datetime.datetime.strptime("00:01", "%H:%M").time()
    morning_end = datetime.datetime.strptime("11:59", "%H:%M").time()
    afternoon_start = datetime.datetime.strptime("12:00", "%H:%M").time()
    afternoon_end = datetime.datetime.strptime("23:59", "%H:%M").time()

    # Check each day and time range
    for day in range(7):  # 0 to 6, representing Monday to Sunday
        if current_day == day:
            if morning_start <= current_time < morning_end:
                return day * 2
            elif afternoon_start <= current_time < afternoon_end:
                return day * 2 + 1

    # If not in any specified range, return -1 or handle as needed
    return 0


# æ­é…worker å±•ç¤ºç»“æœ
def main():
    weekday = get_current_weekday_plus()
    asn = Wanted_ASN[weekday]
    argv_ = sys.argv
    if len(argv_) <= 1:
        run_task(asn)
        return
    else:
        if argv_[1] == "clean":
            keys_to_delete = r.keys(f'*{asn}*')
            # åˆ é™¤è¿™äº›é”®
            if keys_to_delete:
                r.delete(*keys_to_delete)
            # ç§»é™¤snifferx-result hashmapä¸­ç‰¹æœ‰çš„asn æ‰«æç»“æœ
            delete_keys_containing_asn("snifferx-result", asn)
            print(f"æ¸…ç†ä¸Šæ¬¡è¿è¡Œasnæ•°æ®æˆåŠŸ...")
            # å‘é€TGæ¶ˆæ¯å¼€å§‹
            msg_info = f"å¼€å§‹æ‰«æ: ASN{asn},IPv4è§„æ¨¡: {ASN_Map.get(asn).split(',')[1]}"
            telegram_notify = notify.pretty_telegram_notify("ğŸ”ğŸ”Open-Port-Snifferè¿è¡Œå¼€å§‹",
                                                            f"open-port-sniffer asn{asn}",
                                                            msg_info)
            telegram_notify = notify.clean_str_for_tg(telegram_notify)
            success = notify.send_telegram_message(telegram_notify)

            if success:
                print("Start scan message sent successfully!")
            else:
                print("Start scan message failed to send.")


if __name__ == "__main__":
    main()
